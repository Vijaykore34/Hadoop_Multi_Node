# 1.Launch the instance ===> name NN
  2.ubuntu ===> 20
  3.existing secuity group ===> default
  4.Launch the instance
  5.Go in secuity group ===> edit inbound rule ===> keep frist line as it is ===> second line All Traffic 2.MY IP  save rules 



# Connect To the DataCenter with Public Key or Private in case of windows through Putty.
  1.ssh -i Downloads/file.pem ubuntu@public_dns_address or ssh -i nick.pem ubuntu@public ip 

# Update the system
1.sudo apt-get update && sudo apt-get dist-upgrade -y

# Copy public key on to the DataCenter main server
 1.Open another terminal copy past this in that terminal ===> 1.scp -i multi.pem multi.pem ubuntu@public_dns:~/.ssh
2.Go previous terminal and check key received or not ===> cd .ssh/
3. ls
4.cd 

# Create a Hadoop user for accessing HDFS
1.sudo addgroup hadoop
2.sudo adduser hduser --ingroup hadoop 
3.sudo adduser hduser sudo
4.sudo su hduser
5.cd
6.ll

# Create local key
1.ssh-keygen
Enter
Enter
Enter
2.cd .ssh
3.cat id_rsa.pub >> authorized_keys
4.cd
5.ssh localhost
6.exit

# Copy the instance public key (multi.pem) to hduser's directory
1.sudo su
2.cd
2.cp /home/ubuntu/.ssh/multi.pem /home/hduser/.ssh/ or sir did this cp /home/ubuntu/. and again this ===> cp /home/ubuntu/.ssh/nick.pem /home/dhuser/.ssh/
3.chown hduser:hadoop /home/hduser/.ssh/nick.pem
4.exit
5.cd .ssh/
6.ls -l

# Install Java 8 (Open-JDK)
1.sudo apt install openjdk-8-jdk openjdk-8-jre -y
2.cd
3.java -version


# Download and Install Hadoop

1.wget https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.6.tar.gz
2.tar -xzvf hadoop-3.3.6.tar.gz
3.ls
4.sudo mv hadoop-3.3.6 /usr/local/hadoop
5.sudo chown -R hduser:hadoop /usr/local/hadoop

# Set Enviornment Variable
1.this commmand will show were is your java is located ===> readlink -f $(which java)
2.nano ~/.bashrc or nano .bashrc
3.copy from export to /hadoop
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:/usr/local/hadoop/bin/
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

4.source ~/.bashrc or source .bashrc

5.cd /usr/local/hadoop/etc/hadoop/

#Update hadoop-env.sh
1.nano hadoop-env.sh
2.copy from export to /hadoop two lines only
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_LOG_DIR=/var/log/hadoop
3.sudo mkdir /var/log/hadoop/
4.sudo chown hduser:hadoop /var/log/hadoop/ ===> sir used this command
4.sudo chown -R hduser:hadoop /var/log/hadoop ===> given this 

#Disable FireWall iptables
1.sudo iptables -L -n
2.sudo ufw status
3.sudo ufw disable ==> dont fire if not enabled then fire

#Disabling Transparent Hugepage Compaction

1.cat /sys/kernel/mm/transparent_hugepage/defrag

$ 2.sudo nano /etc/init.d/disable-transparent-hugepages
  3.copy from #!/bin/bash to esac

#!/bin/sh
### BEGIN INIT INFO
# Provides:          disable-transparent-hugepages
# Required-Start:    $local_fs
# Required-Stop:
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Disable Linux transparent huge pages
# Description:       Disable Linux transparent huge pages, to improve
#                    database performance.
### END INIT INFO

case $1 in
  start)
    if [ -d /sys/kernel/mm/transparent_hugepage ]; then
      thp_path=/sys/kernel/mm/transparent_hugepage
    elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then
      thp_path=/sys/kernel/mm/redhat_transparent_hugepage
    else
      return 0
    fi

    echo 'never' > ${thp_path}/enabled
    echo 'never' > ${thp_path}/defrag

    unset thp_path
    ;;
esac

$ 4.sudo chmod 755 /etc/init.d/disable-transparent-hugepages

$ 5.sudo update-rc.d disable-transparent-hugepages defaults

>>Restart server ===> Go in instance ===> instance state ===> Reboot instance 
> 1.Connect again to an instance ===> ssh -i nick.pem ubuntu@public ip
  2.su hduser
  3.cd
  4.check the script is working or not ===> sudo nano /etc/init.d/disable-transparent-hugepages
  5.cat /sys/kernel/mm/transparent_hugepage/defrag

# Set Swappiness
 1.sudo sysctl -a | grep vm.swappiness
 2.sudo sysctl vm.swappiness=1

# Configure NTP 
1.timedatectl status
2.timedatectl list-timezones
3.sudo timedatectl set-timezone Asia/Kolkata
4.sudo apt install ntp -y

##Configure SSH Password less logins
copy past this two lines in terminal ===> 1.sudo su -c touch /home/hduser/.ssh/config; echo "Host *\n StrictHostKeyChecking no\n
UserKnownHostsFile=/dev/null" > /home/hduser/.ssh/config
2.sudo su -c touch /home/hduser/.ssh/config;

 This all are optional command ===>
3.cd .ssh
4.ls
5.nano config
6.cd
7.cd .ssh/
8.ls



 3.sudo service ssh restart

# Configure .profile (make sure you are on NN) 
 1.nano .profile
 copy past this line in bootam ===> eval `ssh-agent` ssh-add /home/hduser/.ssh/nick.pem

 2.source .profile

> Go in Instance --> Action ===> security ===> image and templates ===> create image ===> name it as hadoop3 ==> No reboot enable tick ==> create image


----------****** Create a snapshot at this point ******-----------------
Create 4 nodes from this image

#sudo nano /etc/hosts and include these lines:FQDN
172.31.29.56 ip-172-31-29-56.ec2.internal nn
172.31.27.96 ip-172-31-27-96.ec2.internal rm
172.31.18.137 ip-172-31-18-137.ec2.internal 1dn
172.31.23.79 ip-172-31-23-79.ec2.internal 2dn
172.31.16.236 ip-172-31-16-236.ec2.internal 3dn

Do this for all nodes 


# Install and Configure dsh
sudo apt install dsh -y
sudo nano /etc/dsh/machines.list
#localhost
nn
rm
1dn
2dn
3dn
dsh -a uptime
dsh -a source .profile

cd /usr/local/hadoop/etc/hadoop

# Configure masters and slaves
nano masters
rm

nano workers
1dn
2dn
3dn

#Update core-site.xml
nano core-site.xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn:9000</value>
  </property>

#Update hdfs-site.xml on name node 
mkdir -p /usr/local/hadoop/data/hdfs/namenode
nano hdfs-site.xml
<property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
   <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property> 


#Create proper directories on datanode's
dsh -m 1dn,2dn,3dn mkdir -p /usr/local/hadoop/data/hdfs/datanode
 


#Update yarn-site.xml
nano yarn-site.xml
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>rm</value>
  </property>
<property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
  </property>


#Update mapred-site.xml

nano mapred-site.xml
<property>
    <name>mapreduce.jobtracker.address</name>
    <value>rm:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>

sudo chown -R hduser:hadoop $HADOOP_HOME

#SCP all the files
cd /usr/local/hadoop/etc/hadoop
For all nodes
scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers rm:/usr/local/hadoop/etc/hadoop



#Format Namenode

hdfs namenode -format


# Start the cluster
start-dfs.sh
start-yarn.sh
 
dsh -a jps


hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu
hdfs dfs -put hadoop-3.3.4o apt .tar.gz /user/ubuntu
hdfs dfs -ls 
hdfs dfs -ls -R

yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi  5 10
